"""
í†µí•© í¬ë¡¤ëŸ¬ - ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ìˆ˜ì§‘
usage:
    python crawl.py              # ëˆ„ë½ëœ ë°ì´í„°ë§Œ í¬ë¡¤ë§ (ê¸°ë³¸)
    python crawl.py --full       # ì „ì²´ ìƒˆë¡œ í¬ë¡¤ë§
    python crawl.py --retry      # ì‹¤íŒ¨í•œ ë¦¬ì†ŒìŠ¤ë§Œ ì¬ì‹œë„
    python crawl.py --batch 100  # ë°°ì¹˜ í¬ê¸° ì¡°ì •
    python crawl.py --help       # ë„ì›€ë§
"""
import json
import time
import sys
import io
import os
import shutil
import argparse
from datetime import datetime
from pathlib import Path

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(exist_ok=True)

ENHANCED_PATH = DATA_DIR / "resources_enhanced.json"
BACKUP_PATH = DATA_DIR / "resources_enhanced.backup.json"
FAILED_PATH = DATA_DIR / "crawl_failed.json"
LOG_PATH = BASE_DIR / "crawl.log"

# í¬ë¡¤ë§ ëŒ€ìƒ URL (en-us)
BASE_URL = "https://education.minecraft.net"
RESOURCE_LIST_URL = f"{BASE_URL}/en-us/resources"


# â”€â”€â”€ JavaScript ì¶”ì¶œ ì½”ë“œ (12ê°œ í•„ë“œ ëª¨ë‘ ìˆ˜ì§‘) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXTRACT_JS = """() => {
    const result = {};

    // 1. thumbnail_url - og:image ë©”íƒ€íƒœê·¸
    const ogImage = document.querySelector('meta[property="og:image"]');
    const twitterImage = document.querySelector('meta[name="twitter:image"]');
    const metaImage = ogImage?.content || twitterImage?.content;
    if (metaImage) {
        if (metaImage.startsWith('/')) {
            result.thumbnail_url = 'https://education.minecraft.net' + metaImage;
        } else if (metaImage.startsWith('http')) {
            result.thumbnail_url = metaImage;
        } else {
            result.thumbnail_url = 'https://education.minecraft.net/' + metaImage;
        }
    }

    // 2. tags - category-box-list (ì£¼ë¡œ World í˜ì´ì§€)
    const tagUl = document.querySelector('ul.category-box-list');
    if (tagUl) {
        const items = Array.from(tagUl.querySelectorAll('li.item'));
        const tags = items.map(li => li.textContent.trim()).filter(t => t);
        result.tags = tags;
    } else {
        result.tags = [];
    }

    // 3. subjects - ê³¼ëª©
    const subjectLinks = document.querySelectorAll('a[href*="subjects="]');
    result.subjects_list = Array.from(subjectLinks).map(a => a.textContent.trim()).filter(t => t);

    // 4. ages - ëŒ€ìƒ ì—°ë ¹
    const ageLinks = document.querySelectorAll('a[href*="ages="]');
    result.ages = Array.from(ageLinks).map(a => a.textContent.trim()).filter(t => t);

    // 5. skills - ì—­ëŸ‰
    const allHeadings = document.querySelectorAll('h2, h3');
    for (const h of allHeadings) {
        if (h.textContent.trim().toLowerCase() === 'skills') {
            const container = h.closest('div') || h.parentElement;
            if (container) {
                const ul = container.querySelector('ul');
                if (ul) {
                    result.skills = Array.from(ul.querySelectorAll('li'))
                        .map(li => li.textContent.trim()).filter(t => t);
                }
            }
            break;
        }
    }
    if (!result.skills) result.skills = [];

    // 6. estimated_time - ì˜ˆìƒ ì†Œìš” ì‹œê°„
    for (const h of allHeadings) {
        const text = h.textContent.trim().toLowerCase();
        if (text.includes('estimated time') || text.includes('time to complete')) {
            const next = h.nextElementSibling;
            if (next) {
                result.estimated_time = next.textContent.trim();
            } else {
                const container = h.closest('div') || h.parentElement;
                const p = container?.querySelector('p');
                if (p) result.estimated_time = p.textContent.trim();
            }
            break;
        }
    }

    // 7. languages - ì‚¬ìš© ê°€ëŠ¥ ì–¸ì–´
    const langLinks = document.querySelectorAll('a[href*="languages="]');
    result.languages = Array.from(langLinks).map(a => a.textContent.trim()).filter(t => t);

    // 8. submitted_by - ì œì¶œì
    const bodyText = document.body.innerText;
    const submittedMatch = bodyText.match(/Submitted by[:\\s]*([^\\n]+)/i);
    if (submittedMatch) result.submitted_by = submittedMatch[1].trim();

    // 9. updated - ì—…ë°ì´íŠ¸ ë‚ ì§œ
    const updatedMatch = bodyText.match(/Updated[:\\s]*([^\\n]+)/i);
    if (updatedMatch) result.updated = updatedMatch[1].trim();

    // 10. full_description - ì „ì²´ ì„¤ëª…
    const ogDesc = document.querySelector('meta[property="og:description"]');
    if (ogDesc) result.full_description = ogDesc.content;

    // 11. download_url - .mcworld/.zip ë‹¤ìš´ë¡œë“œ ë§í¬
    const allLinks = document.querySelectorAll('a[href]');
    for (const a of allLinks) {
        const href = a.href || '';
        if (href.includes('.mcworld') || href.includes('.zip') || href.includes('/world/')) {
            // "Open in Minecraft" ë˜ëŠ” ë‹¤ìš´ë¡œë“œ ë§í¬
            const text = a.textContent.trim().toLowerCase();
            if (text.includes('open in minecraft') || text.includes('download') 
                || href.includes('.mcworld') || href.includes('.zip')) {
                result.download_url = href;
                break;
            }
        }
    }

    // 12. supporting_files - êµì•ˆ PDF, PPT ë“±
    const fileLinks = document.querySelectorAll(
        'a[href*="lessonsupportfiles"], a[href*="LessonZipFiles"]'
    );
    result.supporting_files = Array.from(fileLinks).map(a => ({
        name: a.textContent.trim(),
        url: a.href
    })).filter(f => f.name && f.url);

    return result;
}"""


def log(msg):
    """ì½˜ì†” + íŒŒì¼ ë¡œê·¸"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{timestamp}] {msg}"
    print(line)
    try:
        with open(LOG_PATH, 'a', encoding='utf-8') as f:
            f.write(line + "\n")
    except:
        pass


def load_resources():
    """ë¦¬ì†ŒìŠ¤ JSON ë¡œë“œ (ë¹ˆ íŒŒì¼/ì—†ëŠ” íŒŒì¼ ë°©ì–´)"""
    if not ENHANCED_PATH.exists():
        log(f"âš ï¸  {ENHANCED_PATH} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    try:
        content = ENHANCED_PATH.read_text(encoding='utf-8').strip()
        if not content:
            log(f"âš ï¸  {ENHANCED_PATH} íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
        resources = json.loads(content)
        if not isinstance(resources, list):
            log(f"âš ï¸  {ENHANCED_PATH}ê°€ ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return None
        return resources
    except json.JSONDecodeError as e:
        log(f"âŒ JSON íŒŒì‹± ì—ëŸ¬: {e}")
        return None


def save_resources(resources):
    """ì•ˆì „í•œ ì €ì¥ (ë°±ì—… í›„ ì €ì¥)"""
    # ë°±ì—…
    if ENHANCED_PATH.exists():
        try:
            shutil.copy2(ENHANCED_PATH, BACKUP_PATH)
        except:
            pass

    with open(ENHANCED_PATH, 'w', encoding='utf-8') as f:
        json.dump(resources, f, ensure_ascii=False, indent=2)


def save_failed(failed_list):
    """ì‹¤íŒ¨ ë¦¬ì†ŒìŠ¤ ì €ì¥"""
    with open(FAILED_PATH, 'w', encoding='utf-8') as f:
        json.dump(failed_list, f, ensure_ascii=False, indent=2)


def find_missing(resources, mode='default'):
    """í¬ë¡¤ë§ì´ í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì¸ë±ìŠ¤ ì°¾ê¸°"""
    missing = []
    for i, r in enumerate(resources):
        if mode == 'full':
            # ì „ì²´ ì¬í¬ë¡¤ë§
            missing.append(i)
        elif mode == 'retry':
            # ì‹¤íŒ¨í•œ ë¦¬ì†ŒìŠ¤ë§Œ
            if r.get('_crawl_failed'):
                missing.append(i)
        else:
            # ê¸°ë³¸: thumbnail_urlì´ ì—†ê±°ë‚˜ tagsê°€ í•œë²ˆë„ ìˆ˜ì§‘ ì•ˆ ëœ ë¦¬ì†ŒìŠ¤
            needs_crawl = (
                not r.get('thumbnail_url')
                or r.get('_crawl_status') is None  # í•œë²ˆë„ í¬ë¡¤ë§ ì•ˆ ë¨
            )
            if needs_crawl:
                missing.append(i)
    return missing


def extract_data(page, url, retries=3):
    """í˜ì´ì§€ì—ì„œ 12ê°œ í•„ë“œ ì¶”ì¶œ"""
    for attempt in range(retries):
        try:
            page.goto(url, timeout=30000, wait_until='domcontentloaded')
            time.sleep(2)  # JS ë Œë”ë§ ëŒ€ê¸°
            break
        except Exception as e:
            if attempt < retries - 1:
                wait_time = 5 * (attempt + 1)  # 5ì´ˆ, 10ì´ˆ ì ì§„ì  ëŒ€ê¸°
                log(f"     âŸ³ ì¬ì‹œë„ ({attempt + 1}/{retries})... {wait_time}ì´ˆ ëŒ€ê¸°")
                time.sleep(wait_time)
                continue
            else:
                return None, str(e)[:120]

    try:
        data = page.evaluate(EXTRACT_JS)
        return data, None
    except Exception as e:
        return None, str(e)[:120]


def apply_data(resource, data):
    """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ë¦¬ì†ŒìŠ¤ì— ì ìš©"""
    fields_updated = []

    # 1. thumbnail_url
    if data.get('thumbnail_url'):
        resource['thumbnail_url'] = data['thumbnail_url']
        fields_updated.append('thumbnail')

    # 2. tags (ë°°ì—´ â†’ ì‰¼í‘œ ë¬¸ìì—´)
    resource['tags'] = ', '.join(data.get('tags', [])) if data.get('tags') else ''
    fields_updated.append('tags')

    # 3. subjects (í¬ë¡¤ë§ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸, ê¸°ì¡´ê°’ ìœ ì§€ ê°€ëŠ¥)
    if data.get('subjects_list'):
        resource['subjects'] = ', '.join(data['subjects_list'])
        fields_updated.append('subjects')

    # 4. ages
    resource['ages'] = ', '.join(data.get('ages', [])) if data.get('ages') else ''
    if data.get('ages'):
        fields_updated.append('ages')

    # 5. skills
    resource['skills'] = ', '.join(data.get('skills', [])) if data.get('skills') else ''
    if data.get('skills'):
        fields_updated.append('skills')

    # 6. estimated_time
    if data.get('estimated_time'):
        resource['estimated_time'] = data['estimated_time']
        fields_updated.append('time')

    # 7. languages
    resource['languages'] = ', '.join(data.get('languages', [])) if data.get('languages') else ''
    if data.get('languages'):
        fields_updated.append('languages')

    # 8. submitted_by
    if data.get('submitted_by'):
        resource['submitted_by'] = data['submitted_by']
        fields_updated.append('submitted')

    # 9. updated
    if data.get('updated'):
        resource['updated'] = data['updated']
        fields_updated.append('updated')

    # 10. full_description
    if data.get('full_description'):
        resource['full_description'] = data['full_description']
        fields_updated.append('desc')

    # 11. download_url
    if data.get('download_url'):
        resource['download_url'] = data['download_url']
        fields_updated.append('download')

    # 12. supporting_files
    if data.get('supporting_files'):
        resource['supporting_files'] = data['supporting_files']
        fields_updated.append('files')

    # í¬ë¡¤ë§ ìƒíƒœ í‘œì‹œ
    resource['_crawl_status'] = 'done'
    resource['_crawl_at'] = datetime.now().isoformat()
    resource.pop('_crawl_failed', None)

    return fields_updated


def format_eta(remaining, avg_time):
    """ë‚¨ì€ ì‹œê°„ í¬ë§·íŒ…"""
    if avg_time <= 0:
        return "ê³„ì‚° ì¤‘..."
    total_sec = remaining * avg_time
    if total_sec < 60:
        return f"{total_sec:.0f}ì´ˆ"
    elif total_sec < 3600:
        return f"{total_sec / 60:.0f}ë¶„"
    else:
        hours = int(total_sec // 3600)
        mins = int((total_sec % 3600) // 60)
        return f"{hours}ì‹œê°„ {mins}ë¶„"


def progress_bar(current, total, width=30):
    """í”„ë¡œê·¸ë ˆìŠ¤ ë°”"""
    pct = current / total if total > 0 else 0
    filled = int(width * pct)
    bar = 'â”' * filled + 'â–‘' * (width - filled)
    return f"{bar} {pct * 100:.1f}%"


def crawl(resources, indices, batch_size=0, delay=3.0, rest_interval=20, rest_duration=30, headless=False):
    """ë©”ì¸ í¬ë¡¤ë§ ë£¨í”„

    Args:
        resources: ì „ì²´ ë¦¬ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
        indices: í¬ë¡¤ë§í•  ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        batch_size: 0ì´ë©´ ì „ì²´, ì–‘ìˆ˜ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œ
        delay: ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
        rest_interval: Nê°œë§ˆë‹¤ íœ´ì‹
        rest_duration: íœ´ì‹ ì‹œê°„ (ì´ˆ)
        headless: headless ëª¨ë“œ (CIìš©)
    """
    if batch_size > 0:
        targets = indices[:batch_size]
    else:
        targets = indices

    total = len(targets)
    if total == 0:
        log("ğŸ‰ í¬ë¡¤ë§í•  ë¦¬ì†ŒìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë‘ ì™„ë£Œ!")
        return

    log(f"ğŸ•·ï¸  í¬ë¡¤ë§ ì‹œì‘: {total}ê°œ ë¦¬ì†ŒìŠ¤")
    log(f"   ë”œë ˆì´: {delay}ì´ˆ, {rest_interval}ê°œë§ˆë‹¤ {rest_duration}ì´ˆ íœ´ì‹")
    log("")

    success_count = 0
    failed_count = 0
    failed_list = []
    start_time = time.time()

    try:
        from playwright.sync_api import sync_playwright

        def create_context(browser):
            """ìƒˆ ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
            ctx = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
            )
            return ctx, ctx.new_page()

        with sync_playwright() as p:
            # CI í™˜ê²½ ìë™ ê°ì§€
            is_headless = headless or os.getenv('CI') == 'true'
            browser_args = ['--disable-http2']
            if is_headless:
                browser_args.append('--no-sandbox')
            
            log(f"ğŸŒ ë¸Œë¼ìš°ì € ëª¨ë“œ: {'headless' if is_headless else 'headed'}")
            browser = p.chromium.launch(
                headless=is_headless,
                args=browser_args
            )
            context, page = create_context(browser)
            consecutive_failures = 0

            for seq, idx in enumerate(targets, 1):
                resource = resources[idx]
                url = resource['url']
                # en-us URL í™•ì¸ (ko-kr â†’ en-us ë³€í™˜)
                if '/ko-kr/' in url:
                    url = url.replace('/ko-kr/', '/en-us/')
                elif '/en-us/' not in url:
                    # education.minecraft.net/worlds/xxx â†’ /en-us/worlds/xxx
                    url = url.replace('education.minecraft.net/', 'education.minecraft.net/en-us/')

                title = resource.get('title', 'Unknown')[:45]
                elapsed = time.time() - start_time
                avg_time = elapsed / seq if seq > 1 else delay + 2
                eta = format_eta(total - seq, avg_time)

                log(f"[{seq}/{total}] {progress_bar(seq, total)} ETA: {eta}")
                log(f"  ğŸ“„ {title}")

                # 50ê°œë§ˆë‹¤ ì»¨í…ìŠ¤íŠ¸ ì¬ìƒì„± (ì—°ê²° ê°±ì‹ )
                if seq > 1 and seq % 50 == 0:
                    log(f"  ğŸ”„ ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ê°±ì‹ ...")
                    try:
                        context.close()
                    except:
                        pass
                    context, page = create_context(browser)
                    time.sleep(3)

                # í¬ë¡¤ë§
                data, error = extract_data(page, url)

                if data:
                    fields = apply_data(resource, data)
                    log(f"  âœ… {', '.join(fields)}")
                    success_count += 1
                    consecutive_failures = 0
                else:
                    log(f"  âŒ {error}")
                    resource['_crawl_failed'] = True
                    resource['_crawl_error'] = error
                    failed_count += 1
                    failed_list.append({
                        'index': idx,
                        'url': url,
                        'title': title,
                        'error': error
                    })
                    consecutive_failures += 1

                    # 5íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ ì»¨í…ìŠ¤íŠ¸ ì¬ìƒì„± + ì¥ì‹œê°„ ëŒ€ê¸°
                    if consecutive_failures >= 5:
                        log(f"  âš ï¸ {consecutive_failures}íšŒ ì—°ì† ì‹¤íŒ¨ - 60ì´ˆ ëŒ€ê¸° í›„ ì»¨í…ìŠ¤íŠ¸ ì¬ìƒì„±")
                        time.sleep(60)
                        try:
                            context.close()
                        except:
                            pass
                        context, page = create_context(browser)
                        consecutive_failures = 0
                        time.sleep(5)

                # ìë™ ì €ì¥ (10ê°œë§ˆë‹¤)
                if seq % 10 == 0:
                    save_resources(resources)
                    log(f"  ğŸ’¾ ìë™ ì €ì¥ ì™„ë£Œ ({seq}/{total})")

                # íœ´ì‹ (rest_intervalë§ˆë‹¤)
                if seq % rest_interval == 0 and seq < total:
                    log(f"  â˜• {rest_duration}ì´ˆ íœ´ì‹...")
                    time.sleep(rest_duration)
                else:
                    time.sleep(delay)

            browser.close()

    except KeyboardInterrupt:
        log("")
        log("âš ï¸  Ctrl+C ê°ì§€ - í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    except Exception as e:
        log(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
    finally:
        # í•­ìƒ ì €ì¥
        save_resources(resources)
        if failed_list:
            save_failed(failed_list)

        elapsed = time.time() - start_time
        processed = success_count + failed_count

        log("")
        log("=" * 60)
        log("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼")
        log("=" * 60)
        log(f"  ì„±ê³µ: {success_count}")
        log(f"  ì‹¤íŒ¨: {failed_count}")
        log(f"  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed / 60:.1f}ë¶„)")
        if processed > 0:
            log(f"  ì†ë„: {processed / elapsed:.2f} ë¦¬ì†ŒìŠ¤/ì´ˆ")
        log(f"  ì „ì²´ ì§„í–‰ë¥ : {len(resources) - len(indices) + success_count}/{len(resources)}")
        log(f"  ğŸ’¾ ì €ì¥: {ENHANCED_PATH}")
        if failed_list:
            log(f"  âŒ ì‹¤íŒ¨ ëª©ë¡: {FAILED_PATH}")
        log("")


def main():
    parser = argparse.ArgumentParser(
        description="ğŸ•·ï¸ Minecraft Education í†µí•© í¬ë¡¤ëŸ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python crawl.py              ëˆ„ë½ëœ ë°ì´í„°ë§Œ í¬ë¡¤ë§
  python crawl.py --full       ì „ì²´ ë¦¬ì†ŒìŠ¤ ì¬í¬ë¡¤ë§
  python crawl.py --retry      ì‹¤íŒ¨í•œ ë¦¬ì†ŒìŠ¤ë§Œ ì¬ì‹œë„
  python crawl.py --batch 50   50ê°œë§Œ í¬ë¡¤ë§
  python crawl.py --delay 5    5ì´ˆ ê°„ê²©ìœ¼ë¡œ í¬ë¡¤ë§
        """
    )
    parser.add_argument('--full', action='store_true',
                        help='ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì¬í¬ë¡¤ë§')
    parser.add_argument('--retry', action='store_true',
                        help='ì‹¤íŒ¨í•œ ë¦¬ì†ŒìŠ¤ë§Œ ì¬ì‹œë„')
    parser.add_argument('--batch', type=int, default=0,
                        help='í¬ë¡¤ë§í•  ê°œìˆ˜ (0=ì „ì²´)')
    parser.add_argument('--delay', type=float, default=3.0,
                        help='ìš”ì²­ ê°„ ë”œë ˆì´ ì´ˆ (ê¸°ë³¸: 3)')
    parser.add_argument('--rest-interval', type=int, default=20,
                        help='Nê°œë§ˆë‹¤ íœ´ì‹ (ê¸°ë³¸: 20)')
    parser.add_argument('--rest-duration', type=int, default=30,
                        help='íœ´ì‹ ì‹œê°„ ì´ˆ (ê¸°ë³¸: 30)')
    parser.add_argument('--headless', action='store_true',
                        help='headless ëª¨ë“œ (CI/ì„œë²„ìš©)')

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ•·ï¸  Minecraft Education í†µí•© í¬ë¡¤ëŸ¬")
    print("   12ê°œ í•„ë“œ ìˆ˜ì§‘: thumbnail, tags, subjects, ages,")
    print("   skills, estimated_time, languages, submitted_by,")
    print("   updated, full_description, download_url, supporting_files")
    print("=" * 60)
    print()

    # ë°ì´í„° ë¡œë“œ
    resources = load_resources()
    if resources is None:
        log("âŒ resources_enhanced.jsonì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        log("   ë¨¼ì € ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        log("   data/resources_enhanced.jsonì— ë¦¬ì†ŒìŠ¤ ëª©ë¡ JSONì„ ë„£ì–´ì£¼ì„¸ìš”.")
        sys.exit(1)

    log(f"ğŸ“Š ì „ì²´ ë¦¬ì†ŒìŠ¤: {len(resources)}ê°œ")

    # í¬ë¡¤ë§ ëŒ€ìƒ ì°¾ê¸°
    mode = 'full' if args.full else ('retry' if args.retry else 'default')
    missing = find_missing(resources, mode)

    # í˜„ì¬ ìƒíƒœ í‘œì‹œ
    has_thumbnail = sum(1 for r in resources if r.get('thumbnail_url'))
    has_tags = sum(1 for r in resources if r.get('tags'))
    has_subjects = sum(1 for r in resources if r.get('subjects'))
    has_ages = sum(1 for r in resources if r.get('ages'))
    has_skills = sum(1 for r in resources if r.get('skills'))
    crawled = sum(1 for r in resources if r.get('_crawl_status') == 'done')

    log(f"   í¬ë¡¤ë§ ì™„ë£Œ: {crawled}ê°œ")
    log(f"   thumbnail: {has_thumbnail}ê°œ")
    log(f"   tags: {has_tags}ê°œ")
    log(f"   subjects: {has_subjects}ê°œ")
    log(f"   ages: {has_ages}ê°œ")
    log(f"   skills: {has_skills}ê°œ")
    log(f"")
    log(f"ğŸ” í¬ë¡¤ë§ ëŒ€ìƒ ({mode}): {len(missing)}ê°œ")
    log("")

    if not missing:
        log("ğŸ‰ í¬ë¡¤ë§í•  ë¦¬ì†ŒìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    # í¬ë¡¤ë§ ì‹œì‘
    crawl(
        resources=resources,
        indices=missing,
        batch_size=args.batch,
        delay=args.delay,
        rest_interval=args.rest_interval,
        rest_duration=args.rest_duration,
        headless=args.headless,
    )


if __name__ == "__main__":
    main()
