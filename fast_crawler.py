"""
ë¹ ë¥¸ í¬ë¡¤ëŸ¬ - requests + BeautifulSoup ì‚¬ìš© (Playwright ì—†ì´)
ì¸ë„¤ì¼ê³¼ íƒœê·¸ëŠ” HTML meta íƒœê·¸ì— ìˆì–´ì„œ JavaScript ë Œë”ë§ ë¶ˆí•„ìš”
"""
import json
import time
import sys
import io
import os
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ë°°ì¹˜ í¬ê¸° ì„¤ì •
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '100'))
MAX_WORKERS = int(os.getenv('MAX_WORKERS', '5'))  # ë™ì‹œ í¬ë¡¤ë§ ê°œìˆ˜


def crawl_resource(url, retries=3):
    """ë‹¨ì¼ ë¦¬ì†ŒìŠ¤ í¬ë¡¤ë§ - requests ì‚¬ìš©"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            data = {}

            # ì¸ë„¤ì¼ - meta íƒœê·¸ì—ì„œ ì¶”ì¶œ
            og_image = soup.find('meta', property='og:image')
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})

            meta_image = None
            if og_image and og_image.get('content'):
                meta_image = og_image['content']
            elif twitter_image and twitter_image.get('content'):
                meta_image = twitter_image['content']

            if meta_image:
                # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if meta_image.startswith('/'):
                    data['thumbnail_url'] = 'https://education.minecraft.net' + meta_image
                elif meta_image.startswith('http'):
                    data['thumbnail_url'] = meta_image
                else:
                    data['thumbnail_url'] = 'https://education.minecraft.net/' + meta_image

            # íƒœê·¸ - h1 ë‹¤ìŒ ulì˜ lië“¤
            h1 = soup.find('h1')
            if h1:
                parent = h1.find_parent()
                if parent:
                    ul = parent.find('ul')
                    if ul:
                        tags = []
                        for li in ul.find_all('li'):
                            tag_text = li.get_text(strip=True)
                            if tag_text and len(tag_text) < 20:
                                tags.append(tag_text)
                        if tags:
                            data['tags'] = tags

            # Submitted by
            body_text = soup.get_text()
            import re
            submitted_match = re.search(r'Submitted by[:\s]*([^\n]+)', body_text, re.IGNORECASE)
            if submitted_match:
                data['submitted_by'] = submitted_match.group(1).strip()

            # Updated
            updated_match = re.search(r'Updated[:\s]*([^\n]+)', body_text, re.IGNORECASE)
            if updated_match:
                data['updated'] = updated_match.group(1).strip()

            return {'success': True, 'data': data, 'url': url}

        except requests.exceptions.RequestException as e:
            if attempt < retries - 1:
                wait_time = 5 * (attempt + 1)
                time.sleep(wait_time)
                continue
            else:
                return {'success': False, 'error': str(e)[:100], 'url': url}

    return {'success': False, 'error': 'Max retries exceeded', 'url': url}


def crawl_batch(resources_to_crawl, max_workers=5):
    """ë°°ì¹˜ í¬ë¡¤ë§ - ë©€í‹°ìŠ¤ë ˆë”© ì‚¬ìš©"""
    results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ì‘ì—… ì œì¶œ
        future_to_resource = {
            executor.submit(crawl_resource, resource['url']): resource
            for resource in resources_to_crawl
        }

        # ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(future_to_resource):
            resource = future_to_resource[future]
            try:
                result = future.result()
                result['resource'] = resource
                results.append(result)

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if result['success']:
                    print(f"âœ… {resource['title'][:50]}")
                    if result['data'].get('thumbnail_url'):
                        print(f"   Thumbnail: {result['data']['thumbnail_url'][:60]}")
                else:
                    print(f"âŒ {resource['title'][:50]} - {result.get('error', 'Unknown error')}")

            except Exception as e:
                print(f"âŒ {resource['title'][:50]} - Exception: {str(e)[:100]}")
                results.append({
                    'success': False,
                    'error': str(e),
                    'url': resource['url'],
                    'resource': resource
                })

    return results


def main():
    print("=" * 70)
    print("ğŸš€ Fast Crawler - requests + BeautifulSoup")
    print("=" * 70)
    print()

    # ë°ì´í„° ë¡œë“œ - resources_complete.json ìš°ì„ , ì—†ìœ¼ë©´ resources_enhanced.json
    output_file = 'data/resources_complete.json'
    if os.path.exists(output_file):
        print(f"ğŸ“‚ Loading from {output_file}")
        with open(output_file, 'r', encoding='utf-8') as f:
            resources = json.load(f)
    else:
        print(f"ğŸ“‚ Loading from data/resources_enhanced.json")
        with open('data/resources_enhanced.json', 'r', encoding='utf-8') as f:
            resources = json.load(f)

    # ëˆ„ë½ëœ ë°ì´í„° ì°¾ê¸°
    missing = [(i, r) for i, r in enumerate(resources) if not r.get('thumbnail_url')]

    print(f"ğŸ“Š Status:")
    print(f"   Total: {len(resources)}")
    print(f"   Complete: {len(resources) - len(missing)} ({(len(resources) - len(missing)) * 100 // len(resources)}%)")
    print(f"   Missing: {len(missing)}")
    print()

    if not missing:
        print("ğŸ‰ All resources complete!")
        return

    # ë°°ì¹˜ ì„¤ì •
    batch = missing[:BATCH_SIZE]
    print(f"ğŸ•·ï¸  Crawling batch: {len(batch)} resources")
    print(f"   Workers: {MAX_WORKERS} concurrent threads")
    print(f"   Remaining: {max(0, len(missing) - BATCH_SIZE)}")
    print()

    # í¬ë¡¤ë§
    resources_to_crawl = [r for i, r in batch]
    start_time = time.time()

    results = crawl_batch(resources_to_crawl, max_workers=MAX_WORKERS)

    elapsed = time.time() - start_time
    print()
    print(f"â±ï¸  Crawling completed in {elapsed:.1f} seconds")
    print()

    # ê²°ê³¼ ì—…ë°ì´íŠ¸
    success_count = 0
    failed_count = 0

    for result in results:
        if result['success'] and result['data'].get('thumbnail_url'):
            resource_url = result['url']
            # resourcesì—ì„œ í•´ë‹¹ URL ì°¾ì•„ì„œ ì—…ë°ì´íŠ¸
            for i, r in enumerate(resources):
                if r['url'] == resource_url:
                    resources[i]['thumbnail_url'] = result['data']['thumbnail_url']
                    if result['data'].get('tags'):
                        resources[i]['tags'] = ', '.join(result['data']['tags'])
                    if result['data'].get('submitted_by'):
                        resources[i]['submitted_by'] = result['data']['submitted_by']
                    if result['data'].get('updated'):
                        resources[i]['updated'] = result['data']['updated']
                    success_count += 1
                    break
        else:
            failed_count += 1

    # ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥
    output_file = 'data/resources_complete.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(resources, f, ensure_ascii=False, indent=2)

    # ê²°ê³¼ ìš”ì•½
    print("=" * 70)
    print("âœ¨ Results")
    print("=" * 70)
    print(f"Success: {success_count}/{len(batch)} ({success_count * 100 // len(batch)}%)")
    print(f"Failed: {failed_count}/{len(batch)}")
    print(f"Speed: {len(batch) / elapsed:.1f} resources/second")
    print(f"Total progress: {len(resources) - len(missing) + success_count}/{len(resources)} ({((len(resources) - len(missing) + success_count) * 100) // len(resources)}%)")
    print()
    print(f"ğŸ’¾ Saved to: {output_file}")
    print()

    # Playwrightì™€ ë¹„êµ
    playwright_time = len(batch) * 10  # PlaywrightëŠ” ë¦¬ì†ŒìŠ¤ë‹¹ ì•½ 10ì´ˆ
    print(f"ğŸ’¡ Time saved vs Playwright: {playwright_time - elapsed:.0f} seconds (~{(playwright_time - elapsed) / 60:.1f} minutes)")
    print()


if __name__ == "__main__":
    main()
